{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AppWebLg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karayolharun/myProjects/blob/master/Pat_Mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMNPrVgzpidW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INSTALLATION - DRIVE MOUNT MODULE ***********************************\n",
        "!python -m spacy download en_core_web_lg\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cTQzbsIaqGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAIN MODULE - REFERENCES-QUERY WORDS-QUERY PRODUCTION-LINK PRODUCTION *******************************************************\n",
        "#------------------------SAVES OUTPUT TO TEXT FILE---------------------------\n",
        "# URLs \n",
        "# REFERENCES\n",
        "# NOUN-PHRASES\n",
        "# QUERY\n",
        "\n",
        "# pyc dosyası üretmek için *********************************\n",
        "#import py_compile\n",
        "#script = \"/content/drive/My Drive/kodlar/kod.py\"\n",
        "#py_compile.compile(script)\n",
        "# **********************************************************\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from os import path\n",
        "\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "\n",
        "f = open('/content/drive/My Drive/MYOUTPUT.txt', 'a')\n",
        "choice_text = input(\"Description or Claim ? : \")\n",
        "if choice_text.lower()==\"d\":\n",
        "  print(\"MYOUTPUT-D\\nINPUT IS DESCRIPTION TEXT\\n\",file=f)\n",
        "  print(\"INPUT IS DESCRIPTION TEXT\\n\")\n",
        "else: \n",
        "  print(\"MYOUTPUT-C\\nINPUT IS CLAIM TEXT\\n\",file=f)\n",
        "  print(\"INPUT IS CLAIM TEXT\\n\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "while True:\n",
        "  oldest_date = input(\"For prior art search insert the priority or filing date in format:yyyymmdd  \")\n",
        " \n",
        "  if int(oldest_date[4:6])>12 or int(oldest_date[6:])>31:\n",
        "    print(\"Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "  \n",
        "  elif (oldest_date[4:6]=='00') or (oldest_date[6:]=='00'):\n",
        "    print(\"Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "      \n",
        "  elif oldest_date[4:6]=='02' and oldest_date[6:]=='30':\n",
        "    print(\"February cannot have 30 days. Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "  \n",
        "  elif oldest_date[4:6]=='02' and oldest_date[6:]=='31':\n",
        "    print(\"February cannot have 31 days. Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "\n",
        "  elif int(oldest_date[:4])%4 != 0 and oldest_date[4:6]=='02' and oldest_date[6:] == '29':\n",
        "    print(\"This is not a leap year. Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "\n",
        "  \n",
        "  elif oldest_date.isnumeric and len(oldest_date)==8:\n",
        "    if int(oldest_date[4:6])<13:\n",
        "      if int(oldest_date[6:])<32:\n",
        "        break  \n",
        "  \n",
        "  else:\n",
        "    print(\"Please enter a valid date in format: yyyymmdd  \")\n",
        "    continue\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "if (choice_text.lower()==\"d\"):\n",
        "  words = input(\"Insert description text here:  \")\n",
        "else:\n",
        "  words = input(\"Insert claim text here:  \")\n",
        "\n",
        "\n",
        "\n",
        "doc = nlp(words)\n",
        "#-------------------------------------------------\n",
        "f = open('/content/drive/My Drive/MYOUTPUT.txt', 'a')\n",
        "\n",
        "#IPC SENTENCE BULMA İŞİ\n",
        "try:\n",
        "  ipc_sent = list()\n",
        "  for sent in doc.sents:\n",
        "    for token in sent:\n",
        "      if (token.text.startswith(\"invent\") or token.text.startswith(\"applicat\")) and token.pos_ == 'NOUN':\n",
        "        if doc[token.i+1].text.startswith(\"relat\") or doc[token.i+2].text.startswith(\"relat\")or doc[token.i+1].text.startswith(\"about\") or doc[token.i+2].text.startswith(\"about\") or doc[token.i+1].text.startswith(\"concern\") or doc[token.i+2].text.startswith(\"concern\"):\n",
        "          print(\"\\nIPC sentence:  \",sent,\"\\n\") #sonra silinebilir\n",
        "          ipc_sent.append(sent)\n",
        "          print(\"ipc_sent listesi  :\",sent,\"\\n\") #sonra silinebilir\n",
        "except:\n",
        "  print(\"\\n NO IPC SENTENCE FOUND \\n\")\n",
        "  print(\"\\n NO IPC SENTENCE FOUND \\n\",file=f)\n",
        "\n",
        "#ipc_sent span listesi*\n",
        "try:\n",
        "  if (len(ipc_sent)>0):\n",
        "    doc_ipc_sent = nlp(str(ipc_sent[0]))\n",
        "    print (\"doc_ipc_sent:  \",doc_ipc_sent)\n",
        "  else:\n",
        "    print(\"\\n NO IPC SENTENCE FOUND \\n\")\n",
        "    print(\"\\n NO IPC SENTENCE FOUND \\n\",file =f)   \n",
        "\n",
        "  ipc_sent_span = doc_ipc_sent\n",
        "  for token in doc_ipc_sent:\n",
        "    if token.text.startswith(\"concern\") or token.text.startswith(\"relat\") or token.text.startswith(\"about\"):\n",
        "      ipc_sent_span = doc_ipc_sent[token.i+1:]\n",
        "      break\n",
        "\n",
        "\n",
        "  ipc_sent_noun_list = list()\n",
        "  for token in ipc_sent_span:\n",
        "    if (token.pos_ == 'ADJ') or (token.pos_ == 'NOUN') or (token.tag_ == 'VBG'):\n",
        "      if  token.text.endswith(\"s\")==True and len(token.text)>3: #ex: \"keys\" sonundaki 's' atılır#\n",
        "        ipc_sent_noun_list.append(str(token)[:-1])\n",
        "      else:\n",
        "        ipc_sent_noun_list.append(str(token))\n",
        "\n",
        "        \n",
        "#ipc_sent_noun_list elemanları string#\n",
        "\n",
        "  ipc_sent_noun_set = set(ipc_sent_noun_list)\n",
        "\n",
        "  ipc_sent_noun_unique_list = list(ipc_sent_noun_set)\n",
        "\n",
        "  print (\"\\n IPC NOUN SET LIST: \",ipc_sent_noun_unique_list)\n",
        "  print (\"\\n IPC NOUN SET LIST: \",ipc_sent_noun_unique_list, file=f)\n",
        "\n",
        "  #https://patents.google.com/?q=(vehicle+seat)&q=(folding+mechanism)&q=lock&before=publication:20140624\n",
        "  ipc_url_noun_addition_string = \"\"\n",
        "  if len(ipc_sent_noun_unique_list)>0:\n",
        "    for i in range(len(ipc_sent_noun_unique_list)):\n",
        "      if i==0:\n",
        "        ipc_url_noun_addition_string += \"?q=(\"+ipc_sent_noun_unique_list[i]+\")\"\n",
        "      else:\n",
        "        ipc_url_noun_addition_string += \"&q=(\"+ipc_sent_noun_unique_list[i]+\")\"\n",
        "    ipc_noun_search_url = \"https://patents.google.com/\"+ipc_url_noun_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"    #IPC SEARCH URL PRODUCED\n",
        "  else:\n",
        "    print(\"\\nNO IPC NOUN SEARCH URL PRODUCED\\n\")\n",
        "    print(\"\\nNO IPC NOUN SEARCH URL PRODUCED\\n\",file=f)\n",
        "\n",
        "\n",
        "\n",
        "  #liste_chunk_ipc , ipc noun-chunk string listesi#\n",
        "  liste_chunk_ipc=[]\n",
        "  for chunk in ipc_sent_span.noun_chunks:\n",
        "      if chunk[0].is_stop == True or chunk[0].text == \"-\" or chunk[0].is_punct == True or chunk[0].pos_ == 'NUM':\n",
        "        if chunk.text.endswith(\"s\"):\n",
        "          liste_chunk_ipc.append(chunk[1:].text.rstrip(\"s\"))\n",
        "        else:\n",
        "          liste_chunk_ipc.append(chunk[1:].text)\n",
        "      else:\n",
        "        if chunk.text.endswith(\"s\"):\n",
        "          liste_chunk_ipc.append(chunk.text.rstrip(\"s\"))\n",
        "        else:\n",
        "          liste_chunk_ipc.append(chunk.text)\n",
        "      \n",
        "  ipc_chunk_set = set(liste_chunk_ipc)\n",
        "\n",
        "  ipc_sent_chunk_unique_list = list(ipc_chunk_set)\n",
        "\n",
        "  print (\"\\n IPC CHUNK SET LIST: \",ipc_sent_chunk_unique_list)\n",
        "  print (\"\\n IPC CHUNK SET LIST: \",ipc_sent_chunk_unique_list, file=f)\n",
        "    \n",
        "    \n",
        "  ipc_url_chunk_addition_string = \"\"\n",
        "  if len(ipc_sent_chunk_unique_list)>0:\n",
        "    for i in range(len(ipc_sent_chunk_unique_list)):\n",
        "      if i==0:\n",
        "        ipc_url_chunk_addition_string += \"?q=(\"+ipc_sent_chunk_unique_list[i].replace(' ','+')+\")\"\n",
        "      else:\n",
        "        ipc_url_chunk_addition_string += \"&q=(\"+ipc_sent_chunk_unique_list[i].replace(' ','+')+\")\"\n",
        "    ipc_chunk_search_url = \"https://patents.google.com/\"+ipc_url_chunk_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"    #IPC SEARCH URL PRODUCED\n",
        "  else:\n",
        "    print(\"\\nNO IPC CHUNK SEARCH URL PRODUCED\\n\")\n",
        "    print(\"\\nNO IPC CHUNK SEARCH URL PRODUCED\\n\",file=f)\n",
        "\n",
        "except:\n",
        "  print(\"\\nNO IPC SENTENCE AND DOC OBJECT\\n\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "#----------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#for ent in doc.ents:\n",
        "#  print(\"ents: \", ent.text, \" \", ent.label_)\n",
        "\n",
        "#nlp.tokenizer.add_special_case('-', [{ORTH: '-', POS:ADP}])\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "\n",
        "#pattern 00,01,10,11 : referanslar için (ör: a chassis, the chassis, vehicle chassis)\n",
        "pattern00 = [{'POS': 'DET', 'OP': '+'},{'POS':'NOUN'},{'LIKE_NUM': True,'IS_DIGIT': True}]\n",
        "pattern01 = [{'POS':'NOUN'},{'POS':'NOUN'},{'LIKE_NUM': True,'IS_DIGIT': True}]\n",
        "pattern02 = [{'POS':'ADJ'},{'POS':'NOUN'},{'LIKE_NUM': True,'IS_DIGIT': True}]\n",
        "pattern03 = [{'LIKE_NUM': True,'IS_DIGIT': False}, {'POS':'NOUN'},{'LIKE_NUM': True,'IS_DIGIT': True}]\n",
        "pattern10 = [{'POS': 'DET', 'OP': '+'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True,'IS_DIGIT': True},{'TEXT':')'}]\n",
        "pattern11 = [{'POS':'NOUN'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True,'IS_DIGIT': True},{'TEXT':')'}]\n",
        "pattern12 = [{'POS':'ADJ'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True,'IS_DIGIT': True},{'TEXT':')'}]\n",
        "pattern13 = [{'LIKE_NUM': True,'IS_DIGIT': False}, {'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True,'IS_DIGIT': True},{'TEXT':')'}]\n",
        "\n",
        "#pattern 2,3,4(?) : noun ve noun_phrase için sonrasında frekans belirlenerek SEARCH query keyword belirlemede kullanılıyor\n",
        "\n",
        "pattern2 = [{'LIKE_NUM': True,'IS_DIGIT': False}, {'POS':'NOUN'}]\n",
        "pattern3 = [{'POS':'NOUN'},{'POS':'NOUN'}]\n",
        "#pattern4 = [{'POS':'ADJ'},{'POS':'NOUN'}]\n",
        "\n",
        "matcher.add('REFERENCE',None,pattern00,pattern01,pattern02,pattern03,pattern10,pattern11,pattern12,pattern13)\n",
        "found_matches = matcher(doc)\n",
        "\n",
        "f = open('/content/drive/My Drive/MYOUTPUT.txt', 'a')\n",
        "\n",
        "print(\"\\nPOSSIBLE REFERENCES : \\n\",file=f)\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text,file=f)\n",
        "\n",
        "print('\\n',file=f)\n",
        "\n",
        "\n",
        "if len(found_matches)==0:\n",
        "  print(\"NO POSSIBLE REFERENCE found with given format\\n\",file=f)\n",
        "  print(\"NO POSSIBLE REFERENCE found with given format\\n\")\n",
        "\n",
        "\n",
        "referenceList = [doc[start:end].text for match_id, start, end in found_matches]\n",
        "referenceList_freq = Counter(referenceList)\n",
        "common_words = referenceList_freq.most_common(40)\n",
        "print(\"\\nPOSSIBLE REFERENCES (reference_phrase,freq): \", common_words,\"\\n\", file=f)\n",
        "\n",
        "print(\"SORTED POSSIBLE REFERENCES REGARDING FREQUENCY IN DESCENDING ORDER: \\n\",file=f)\n",
        "print(\"SORTED POSSIBLE REFERENCES REGARDING FREQUENCY IN DESCENDING ORDER: \\n\")\n",
        "\n",
        "\n",
        "sorted_reference_list = [word for (word, freq) in common_words]\n",
        "for i in range(0,len(sorted_reference_list)):\n",
        "  print(str(i+1)+\")-  \"+sorted_reference_list[i],file=f)\n",
        "  print(str(i+1)+\")-  \"+sorted_reference_list[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "matcher.remove('REFERENCE')\n",
        "matcher.add('NOUN_PHRASE',None,pattern2,pattern3)\n",
        "found_matches = matcher(doc)\n",
        "print(\"\\nPOSSIBLE NOUN PHRASES:\\n\", file=f)\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text,file=f)\n",
        "    \n",
        "    \n",
        "startDocText = [doc[start:end].text for match_id, start, end in found_matches]\n",
        "\n",
        "\n",
        "unexpected_element=[]\n",
        "try:\n",
        "  for i in range(0,len(startDocText)):\n",
        "    if \"-\" in startDocText[i]:\n",
        "      #print(\"unexpected element = \")\n",
        "      unexpected_element.append(startDocText[i])\n",
        "except IndexError:\n",
        "  print(\"ERROR:index out of range\",file=f)\n",
        "\n",
        "print(\"List for unexpected element:\",unexpected_element,file=f)\n",
        "\n",
        "startDocText = [x for x in startDocText if x not in unexpected_element]\n",
        "\n",
        "\n",
        "print(\"\\nCRITICAL IPC KEYWORDS (comprising system,arrangement,product,device,method etc.):\\n\",file=f)\n",
        "print(\"\\nCRITICAL IPC KEYWORDS (comprising system,arrangement,product,device,method etc.):\\n\")\n",
        "\n",
        "\n",
        "try:\n",
        "    \n",
        "  for i in range(len(startDocText)):\n",
        "    if (startDocText[i].split()[1].lower()== \"system\" or\n",
        "        startDocText[i].split()[1].lower()==\"arrangement\" or\n",
        "        startDocText[i].split()[1].lower()==\"arangement\" or startDocText[i].split()[1].lower()==\"apparatus\" or\n",
        "        startDocText[i].split()[1].lower()==\"aparatus\" or startDocText[i].split()[1].lower()==\"device\" or\n",
        "        startDocText[i].split()[1].lower()==\"devices\" or\n",
        "        startDocText[i].split()[1].lower()==\"method\" or startDocText[i].split()[1].lower()==\"product\" or \n",
        "        startDocText[i].split()[1].lower()==\"production\" or startDocText[i].split()[1].lower()==\"assembly\" or\n",
        "        startDocText[i].split()[1].lower()==\"asembly\") :\n",
        "      print(startDocText[i],file=f)\n",
        "      print(startDocText[i])\n",
        "      \n",
        "\n",
        "except: \n",
        "  print(\"AN EXCEPTION OCCURED. UNEXPECTED CHARACTER-INDEX ERROR\\n\",file=f)\n",
        "\n",
        "  \n",
        "startDocText_freq = Counter(startDocText)\n",
        "common_words = startDocText_freq.most_common(40)\n",
        "print(\"\\nIPC KEYWORDS (noun_phrase,freq): \", common_words,\"\\n\",file=f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#((vehicle seat)) ((folding mechanism)) (lock) before:publication:20140624\n",
        "\n",
        "#https://patents.google.com/?q=(vehicle+seat)&q=(folding+mechanism)&q=lock&before=publication:20140624\n",
        "#*************** common_words liste lengthi 2'den küçükse \"list index out of range\" hatası veriyor. Dolayısıyla for ile len(list) şeklinde sınırlandırmak doğru olabilir\n",
        "print(\"\\nSORTED IPC KEYWORDS QUERY FORMAT REGARDING FREQUENCY IN DESCENDING ORDER:\\n\",file=f)\n",
        "print(\"\\nSORTED IPC KEYWORDS QUERY FORMAT REGARDING FREQUENCY IN DESCENDING ORDER:\\n\")\n",
        "\n",
        "\n",
        "query_words=[]\n",
        "try:\n",
        "  for i in range(0,len(common_words)):\n",
        "    #print(startDocText[i][0],startDocText[i][-1])\n",
        "    #if (startDocText[i][0] != '-' or startDocText[i][-1] != '-'):\n",
        "    phrase_firstword = common_words[i][0].split()[0].lower()\n",
        "    phrase_secondword = common_words[i][0].split()[1].lower()\n",
        "    if (phrase_secondword[-1]==\"s\"):\n",
        "      phrase_secondword = common_words[i][0].split()[1][:-1].lower()\n",
        "    query_word = phrase_firstword+\"_\"+phrase_secondword+\"+\"\n",
        "    query_words.append(query_word)\n",
        "    print(str(i+1)+\")-  \"+query_word,file=f)\n",
        "    print(str(i+1)+\")-  \"+query_word)\n",
        "\n",
        "    \n",
        "except:\n",
        "  print('ERROR - UNEXPECTED CHARACTER-INDEX ERROR\\n',file=f)\n",
        "\n",
        "\n",
        "unique_keyword_list = [word for (word, freq) in common_words if freq is 1]\n",
        "print(\"\\nUNIQUE KEYWORDS (freq=1): \\n\",file=f)\n",
        "print(\"\\nUNIQUE KEYWORDS (freq=1): \\n\")\n",
        "\n",
        "if (len(unique_keyword_list) ==0):\n",
        "  print(\"NO UNIQUE KEYWORDS FOUND\",file=f)\n",
        "  print(\"NO UNIQUE KEYWORDS FOUND\")\n",
        "\n",
        "else:\n",
        "  for i in range(len(unique_keyword_list)):\n",
        "    print(unique_keyword_list[i],file=f)\n",
        "    print(unique_keyword_list[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  \n",
        "print(\"\\nPROBABLE PATENW QUERY FOR IPC OR SIMILAR DOCUMENTS : \\n\",file=f)\n",
        "print(\"\\nPROBABLE PATENW QUERY FOR IPC OR SIMILAR DOCUMENTS : \\n\")\n",
        "\n",
        "try:\n",
        "  patenw_query = \" \"\n",
        "  if (len(query_words) > 3):\n",
        "    for i in range(0,3):\n",
        "      patenw_query += query_words[i]+\",\"\n",
        "    print(\"and\"+patenw_query[:-1],file=f)\n",
        "    print(\"and\"+patenw_query[:-1])\n",
        "    url = \"https://patents.google.com/?q=({})&q=({})&q=({})&before=publication:{}\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1],query_words[2].replace('_','+')[:-1],oldest_date)+\"&clustered=true\"\n",
        "    print(\"\\nFREQUENCY SEARCH URL: \",url)\n",
        "    print(\"\\nFREQUENCY SEARCH URL: \",url,file=f)\n",
        "    compound_noun_url = \"https://patents.google.com/?q=({})&q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1],query_words[2].replace('_','+')[:-1])+ipc_url_noun_addition_string.replace('?','&')+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "    print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url)   \n",
        "    print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url,file=f)  \n",
        "    compound_chunk_url = \"https://patents.google.com/?q=({})&q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1],query_words[2].replace('_','+')[:-1])+ipc_url_chunk_addition_string.replace('?','&')+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "    print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url)   \n",
        "    print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url,file=f)  \n",
        "\n",
        "#ipc_search_url = \"https://patents.google.com/\"+ipc_url_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "\n",
        "\n",
        "  elif (len(query_words) > 0):\n",
        "    for i in range(0,len(query_words)):\n",
        "      patenw_query += query_words[i]+\",\"\n",
        "    print(\"and\"+patenw_query[:-1],file=f)\n",
        "    print(\"and\"+patenw_query[:-1])\n",
        "    if (len(query_words) == 1):\n",
        "      url = \"https://patents.google.com/?q=({})&before=publication:{}\".format(query_words[0].replace('_','+')[:-1],oldest_date)+\"&clustered=true\"\n",
        "      print(\"\\nFREQUENCY SEARCH URL: \",url)\n",
        "      print(\"\\nFREQUENCY SEARCH URL: \",url,file=f)\n",
        "      compound_noun_url = \"https://patents.google.com/?q=({})&q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1])+ipc_url_noun_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "      print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url)   \n",
        "      print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url,file=f)   \n",
        "      compound_chunk_url = \"https://patents.google.com/?q=({})&q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1])+ipc_url_chunk_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "      print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url)   \n",
        "      print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url,file=f)  \n",
        "\n",
        "    elif (len(query_words) == 2):\n",
        "      url = \"https://patents.google.com/?q=({})&q=({})&before=publication:{}\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1],oldest_date)+\"&clustered=true\"\n",
        "      print(\"\\nFREQUENCY SEARCH URL: \",url)\n",
        "      print(\"\\nFREQUENCY SEARCH URL: \",url,file=f)\n",
        "      compound_noun_url = \"https://patents.google.com/?q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1])+ipc_url_noun_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "      print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url)   \n",
        "      print(\"\\nCOMPOUND NOUN SEARCH URL: \",compound_noun_url,file=f)     \n",
        "      compound_chunk_url = \"https://patents.google.com/?q=({})&q=({})\".format(query_words[0].replace('_','+')[:-1],query_words[1].replace('_','+')[:-1])+ipc_url_chunk_addition_string+\"&before=publication:\"+oldest_date+\"&clustered=true\"\n",
        "      print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url)   \n",
        "      print(\"\\nCOMPOUND CHUNK SEARCH URL: \",compound_chunk_url,file=f)    \n",
        "\n",
        "\n",
        "  else: \n",
        "    print(\"\"\"NO NOUN PHRASE FOUND... NO RESULTS FOR QUERY !\"\"\",file=f)\n",
        "    print(\"\\nNO FREQUENCY SEARCH URL\\n\",file=f)\n",
        "    print(\"\\nNO COMPOUND SEARCH URL ONLY IPC SEARCH URL PRODUCED\\n\",file=f)\n",
        "\n",
        "    print(\"\"\"NO NOUN PHRASE FOUND... NO RESULTS FOR QUERY !\"\"\")\n",
        "    print(\"\\nNO FREQUENCY SEARCH URL\\n\")\n",
        "    print(\"\\nNO COMPOUND SEARCH URL ONLY IPC SEARCH URL PRODUCED\\n\")\n",
        "\n",
        "  print(\"\\nIPC NOUN SEARCH URL: \",ipc_noun_search_url,file=f)\n",
        "  print(\"\\nIPC NOUN SEARCH URL: \",ipc_noun_search_url)\n",
        "  print(\"\\nIPC CHUNK SEARCH URL: \",ipc_chunk_search_url,file=f)\n",
        "  print(\"\\nIPC CHUNK SEARCH URL: \",ipc_chunk_search_url)\n",
        "\n",
        "except:\n",
        "  print(\"\\nNO COMPOUND AND IPC URL ONLY FREQUENCY URL EXISTS\\n\")\n",
        "\n",
        "if choice_text.lower()==\"d\":\n",
        "  os.rename('/content/drive/My Drive/MYOUTPUT.txt','/content/drive/My Drive/MYOUTPUT-D.txt')\n",
        " \n",
        "else: \n",
        "  os.rename('/content/drive/My Drive/MYOUTPUT.txt','/content/drive/My Drive/MYOUTPUT-C.txt') \n",
        "\n",
        "  \n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqiJ49h_xSCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GOOGLE DOCUMENT LIST MODULE******************************\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "# url= 'https://patents.google.com/?q=%28base+lock%29&q=%28lock+folding%29&q=%28folding+mechanism%29&q=%28base+lock+mechanism%29&q=%28seat+lock+folding+mechanism%29&q=%28vehicle+rear+seat%29&q=B60N2%252f00&q=weight+housing&q=rotation+bracket&q=pivoting+bracket&q=bolt&q=bearing&before=publication:20140627&clustered=true'\n",
        "df = pd.read_csv('/content/drive/My Drive/gps.csv')\n",
        "\n",
        "# df.index  --> ('DE-202009002580-U1', ...), alt alta hep böyle\n",
        "\n",
        "#df[url].index[0][0] # 'id'\n",
        "#df[url].index[1][0] #'CN-103380025-A'\n",
        "#df[url].index[2][0] #'WO-9911488-A1'\n",
        "#df[url].index[3][0] #'DE-10127067-A1'\n",
        "\n",
        "range_documents=int(input(\"Enter number of documents+1: \"))\n",
        "for i in range(1,range_documents):\n",
        "  print(str(df['search URL:'].index[i][0]).replace('-','')) \n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0K7rV-cdgIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SIMILAR WORDS MODULE************************************\n",
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "wordforSimilarity= input(\"Insert a word for similar matches: \")\n",
        "wordforSimilarity = wordforSimilarity.lower()\n",
        "new_vector = nlp.vocab[wordforSimilarity].vector\n",
        "\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors and mixed-case words:\n",
        "    if word.vector_norm > 0:\n",
        "        if word.is_lower:\n",
        "          if word.is_alpha:\n",
        "            similarity = cosine_similarity(new_vector, word.vector)\n",
        "            computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "\n",
        "print([w[0].text for w in computed_similarities[:20]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeWfHSrK-8FK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC-PREDICT-NOUN PHRASE\n",
        "#2 AŞAMALI- 3 HANELİ IPC\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "\n",
        "input_doc = nlp(my_input)\n",
        "\n",
        "#for ent in doc.ents:\n",
        "#  print(\"ents: \", ent.text, \" \", ent.label_)\n",
        "\n",
        "#nlp.tokenizer.add_special_case('-', [{ORTH: '-', POS:ADP}])\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "#pattern 00,01,10,11 : referanslar için (ör: a chassis, the chassis, vehicle chassis)\n",
        "patternipc1 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc2 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc3 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'ADJ'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc4 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'ADJ'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc5 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "matcher.add('IPC',None,patternipc1,patternipc2,patternipc3,patternipc4,patternipc5)\n",
        "found_matches = matcher(input_doc)\n",
        "\n",
        "print('\\nPOSSIBLEIPC WORDS:\\n')\n",
        "if len(found_matches)==0:\n",
        "  print(\"NO POSSIBLE IPC WORD found with given format\\n\")\n",
        "\n",
        "list_ipcwords = []\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = input_doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text)\n",
        "  list_ipcwords.append(span.text)\n",
        "print('\\n')\n",
        "\n",
        "ipctext = ' '.join(list_ipcwords)\n",
        "rawdoc = nlp(ipctext)\n",
        "apptext=[]\n",
        "for token in rawdoc:\n",
        "  if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
        "    apptext.append(token.text)\n",
        "# unique ifadeler bulunup listeye dahil edilir, listeden tek bir string(unique_text) üretilir,bu string de nlp ile doc objesine dönüştürülür \n",
        "unique_list = [] \n",
        "for x in apptext:\n",
        "  if x not in unique_list: \n",
        "    unique_list.append(x) \n",
        "\n",
        "unique_text = ' '.join(unique_list)\n",
        "appdoc = nlp(unique_text)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ipc.tsv', sep='\\t')\n",
        "\n",
        "# ilk cümle başarısız olursa alttaki 2 yorum koduna devam edilebilir\n",
        "#app = input(\"insert critical words for ipc: \")\n",
        "#appdoc= nlp(app)\n",
        "probIpc=[]\n",
        "for i in(range(len(df['Definition']))):\n",
        "  print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "\n",
        "print(\"FIRST FIVE MAXIMUM VALUES:\\n\")\n",
        "\n",
        "df['predictionValue']=pd.Series(probIpc)\n",
        "print(df.sort_values(by='predictionValue', ascending = False))\n",
        "print(\"\\n\")\n",
        "\n",
        "try:\n",
        "  print(\"VALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "except:\n",
        "  print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "\n",
        "answer=input(\"\\nIf you are not satisfied with the results do you want to insert your own noun phrases for ipc ?\\n\")\n",
        "\n",
        "if answer.lower() == \"y\" or answer.lower() == \"yes\":\n",
        "  app = input(\"Insert critical noun phrases for ipc wihout punctuation (e.g image processor, marine, communication network ): \")\n",
        "  appdoc= nlp(app)\n",
        "  probIpc=[]\n",
        "  for i in(range(len(df['Definition']))):\n",
        "    print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "    probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  print(\"\\nMAXIMUM VALUES:\\n\")\n",
        "  df['predictionValue']=pd.Series(probIpc)\n",
        "  print(df.sort_values(by='predictionValue', ascending = False).head(20))\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  try:\n",
        "    print(\"VALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "    print(\"\\nYou can run again for new search...\")\n",
        "  except:\n",
        "    print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "else:\n",
        "  print(\"\\nYou can run again for new search...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHr7fl-fsmf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SIMILARITY EXERCISE\n",
        "app=input(\"app: \")\n",
        "ipc=input(\"ipc: \")\n",
        "docipc = nlp(ipc)\n",
        "docapp = nlp(app)\n",
        "print(docapp.vector_norm,docipc.vector_norm)\n",
        "print(\"app:\",docapp.has_vector)\n",
        "print(\"ipc:\",docipc.has_vector)\n",
        "print(docapp.similarity(docipc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyWwAMHtPOPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TOKEN ATTRIBUTES EXERCISE\n",
        "entry=input(\"Bir cümle yaz: \")\n",
        "doc=nlp(entry.lower())\n",
        "for chunk in doc.noun_chunks:\n",
        "  liste=[chunk.text for chunk in doc.noun_chunks]\n",
        "print(\"liste: \",liste)\n",
        "for token in doc:\n",
        "  print(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop,token.like_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhtaEqr6Qw0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC PREDICT-NOUN-CHUNKS (+ (TOKEN.TAG_=VBG) ?)\n",
        "# 2 AŞAMALI\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "#--------\n",
        "input_doc = nlp(my_input)\n",
        "liste_chunk=[]\n",
        "for chunk in input_doc.noun_chunks:\n",
        "  print(\"CHUNK: \",chunk)\n",
        "  for i in(range(len(chunk))):\n",
        "    if (chunk[i].is_stop == False and chunk[i].text != \"-\" and chunk[i].is_punct == False and chunk[i].pos_ != 'NUM'):\n",
        "      liste_chunk.append(chunk[i].text)\n",
        "#---------\n",
        "\n",
        "print(\"CHUNK LİSTESİ: \",liste_chunk)\n",
        "\n",
        "ipctext = ' '.join(liste_chunk)\n",
        "\n",
        "\n",
        "appdoc = nlp(ipctext)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ipc.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "probIpc=[]\n",
        "for i in(range(len(df['Definition']))):\n",
        "  print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES: \\n\")\n",
        "\n",
        "df['predictionValue']=pd.Series(probIpc)\n",
        "print(df.sort_values([\"predictionValue\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#burası çok önemli yeni df'nin indexi sıfırlanmalı\n",
        "df2 = df.sort_values([\"predictionValue\"], ascending = False)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "\n",
        "#most prop ipc list, df2'deki IPC verileri ile oluşturulur\n",
        "most_prob_ipc_list=[]\n",
        "for i in range(0,20):\n",
        "  most_prob_ipc_list.append((df2['IPC'][i],i))\n",
        "\n",
        "final_ipc_df1 = pd.DataFrame(most_prob_ipc_list, columns =['IPC','FIRST-INDEX'])\n",
        "#print(\"MOST PROBABLE IPC LIST: \\n\",most_prob_ipc_list)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"\\nVALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "except:\n",
        "  print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "\n",
        "answer=input(\"\\nIf you are not happy with the results do you want to insert your own noun phrases for ipc ?\\n\")\n",
        "\n",
        "if answer.lower() == \"y\" or answer.lower() == \"yes\":\n",
        "  app = input(\"Insert critical noun phrases for ipc wihout punctuation (e.g image processor, marine, communication network ): \")\n",
        "  appdoc= nlp(app)\n",
        "  probIpc=[]\n",
        "  for i in(range(len(df['Definition']))):\n",
        "    print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "    probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  print(\"\\nMAXIMUM VALUES:\\n\")\n",
        "  df['predictionValue']=pd.Series(probIpc)\n",
        "  print(df.sort_values([\"predictionValue\"], ascending = False).head(20))\n",
        "\n",
        "  print(\"\\nFIRST PREDICTION VALUES:\\n\", df2.head(20))\n",
        "  \n",
        "  # ilk df2'dn farklı olarak ikinci df2 oluşturulur sonuçta her iki df2'deki IPC verileri ile most prob ipc list oluşur\n",
        "  df2=df.sort_values([\"predictionValue\"], ascending = False)\n",
        "  df2 = df2.reset_index(drop=True)\n",
        "\n",
        "  most_prob_ipc_list2=[]\n",
        "  for m in range(0,20):\n",
        "    most_prob_ipc_list2.append((df2['IPC'][m],m))\n",
        "  #print(\"MOST PROBABLE IPC LIST: \\n\",most_prob_ipc_list)\n",
        "  final_ipc_df2 = pd.DataFrame(most_prob_ipc_list2, columns =['IPC','SECOND-INDEX'])\n",
        "  result_df = pd.merge(final_ipc_df1,final_ipc_df2, on='IPC', how='outer', indicator=True)\n",
        "\n",
        "  #result_df = pd.merge(final_ipc_df1,final_ipc_df2, on='IPC')\n",
        "  #result_df = final_ipc_df1.append(final_ipc_df2, sort=False)\n",
        "  print(\"\\nRESULT:\\n\",result_df)\n",
        "  #final_ipc_list = [(x, most_prob_ipc_list.count(x),most_prob_ipc_list.) for x in set(most_prob_ipc_list)]\n",
        "  #print (\"IPC FREQUENCY IN BOTH METHODS: \", final_ipc_list) \n",
        "  \n",
        "  #final_ipc_df = pd.DataFrame(final_ipc_list, columns =['IPC', 'FREQUENCY', 'FIRST-INDEX']) \n",
        "  #-----alttaki komut önemli, son sorting\n",
        "  result_df['TOTAL-INDEX'] = result_df['FIRST-INDEX']+result_df['SECOND-INDEX']\n",
        "\n",
        "  final_sorted_ipc_df = result_df.sort_values([\"TOTAL-INDEX\"], ascending = True)\n",
        "  print(\"\\nIPC FREQUENCY IN BOTH METHODS:\\n \",final_sorted_ipc_df)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nVALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "    print(\"\\nYou can run again for new search...\")\n",
        "  except:\n",
        "    print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "else:\n",
        "  print(\"\\nYou can run again for new search...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3n2ViiFqrfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC PREDICT-NOUN-CHUNKS-FULL IPC- 2 KUTUCUKLU DENEME\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input = my_input.lower()\n",
        "\n",
        "#--------\n",
        "input_doc = nlp(my_input)\n",
        "\n",
        "\n",
        "liste_chunk=[]\n",
        "for chunk in input_doc.noun_chunks:\n",
        "  print(\"CHUNK: \",chunk)\n",
        "  for i in(range(len(chunk))):\n",
        "    if (chunk[i].is_stop == False and chunk[i].text != \"-\" and chunk[i].is_punct == False and chunk[i].pos_ != 'NUM'):\n",
        "      liste_chunk.append(chunk[i].text)\n",
        "#---------\n",
        "\n",
        "\n",
        "print(\"CHUNK LİSTESİ: \",liste_chunk)\n",
        "\n",
        "most_freq_tuple_list = Counter(liste_chunk).most_common(10)\n",
        "most_freq_noun_list = [ word for word,freq in most_freq_tuple_list]\n",
        "print(\"MOST FREQ NOUNS: \",most_freq_noun_list)\n",
        "\n",
        "#---------------------------------------------\n",
        "#burada en yüksek frekanslı kelimelerle bir tex oluşturulur ve sonrasında bu doc objesine dönüştürülür\n",
        "ipctext = ' '.join(most_freq_noun_list)\n",
        "\n",
        "appdoc = nlp(ipctext)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "#--------------------------------------------\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/FULLIPCVEHICLE.tsv', sep='\\t')\n",
        "\n",
        "#BURADA DEF1 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC1 LİSTESİNE EKLENİR. DEF1 4 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc1=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "  probIpc1.append(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF1: \\n\")\n",
        "\n",
        "df['predictionValue1']=pd.Series(probIpc1)\n",
        "print(df.sort_values([\"predictionValue1\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#BURADA DEF2 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC2 LİSTESİNE EKLENİR. DEF2 3 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc2=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "  probIpc2.append(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF2: \\n\")\n",
        "\n",
        "df['predictionValue2']=pd.Series(probIpc2)\n",
        "print(df.sort_values([\"predictionValue2\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#burası çok önemli yeni df'nin indexi sıfırlanmalı\n",
        "df['FULLIPC-Prediction']=df['predictionValue1']+df['predictionValue2']\n",
        "df2 = df.sort_values([\"FULLIPC-Prediction\"], ascending = False)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "print(\"FULL IPC PREDICTION MAX. VALUES:\\n\",df2)\n",
        "\n",
        "# GP-search terms example((side airbag)) ((trim piece)) ((seatback)) before:publication:20140402\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2PiPQO6Efyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC PREDICT-TOKENS-FULL IPC - 2 KUTUCUKLU DENEME - 3 KUTUCUK ?\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "#--------\n",
        "input_doc = nlp(my_input)\n",
        "\n",
        "#--------------------\n",
        "#burada token listesi isim ve 'ing' ile biten isimfiilden oluşacak şekilde üretilir\n",
        "\n",
        "token_list=[]\n",
        "for token in input_doc:\n",
        "  if token.pos_== 'NOUN' or token.tag_=='VBG':\n",
        "    if not token.text in ['invention','product','method','system','prior','mechanism','apparatus','device','embodiment','art','state']: \n",
        "      token_list.append(token.text)\n",
        "\n",
        "most_freq_tuple_list = Counter(token_list).most_common(10)\n",
        "print(\"FREKANSLI LİSTE: \\n\",most_freq_tuple_list)\n",
        "most_freq_noun_list = [ word for word,freq in most_freq_tuple_list]\n",
        "print(\"MOST FREQ NOUNS: \", most_freq_noun_list)\n",
        "#--------------------\n",
        "\n",
        "#---------------------------------------------\n",
        "#burada en yüksek frekanslı kelimelerle bir tex oluşturulur ve sonrasında bu doc objesine dönüştürülür\n",
        "ipctext = ' '.join(most_freq_noun_list)\n",
        "\n",
        "appdoc = nlp(ipctext)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "#---------------------------------------------\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/FULLIPCVEHICLE.tsv', sep='\\t')\n",
        "\n",
        "#BURADA DEF1 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC1 LİSTESİNE EKLENİR. DEF1 4 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc1=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "  probIpc1.append(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF1: \\n\")\n",
        "\n",
        "df['predictionValue1']=pd.Series(probIpc1)\n",
        "print(df.sort_values([\"predictionValue1\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#BURADA DEF2 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC2 LİSTESİNE EKLENİR. DEF2 3 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc2=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "  probIpc2.append(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF2: \\n\")\n",
        "\n",
        "df['predictionValue2']=pd.Series(probIpc2)\n",
        "print(df.sort_values([\"predictionValue2\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#burası çok önemli yeni df'nin indexi sıfırlanmalı\n",
        "df['FULLIPC-Prediction']=df['predictionValue1']+df['predictionValue2']\n",
        "df2 = df.sort_values([\"FULLIPC-Prediction\"], ascending = False)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "print(\"FULL IPC PREDICTION MAX. VALUES:\\n\",df2)\n",
        "\n",
        "# GP-search terms example:((side airbag)) ((trim piece)) ((seatback)) before:publication:20140402\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}