{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebLg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMNPrVgzpidW",
        "colab_type": "code",
        "outputId": "4173396e-22c1-4dec-a9c0-022b6c4180c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=fc654b50d7baf57336210db98a5c23d60c7b1e33be46643f4eb94df76c957fe0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-63b6qy5o/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dac4Uu2BbtTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import py_compile\n",
        "\n",
        "\n",
        "#script = \"/content/drive/My Drive/kodlar/kod.py\"\n",
        "#py_compile.compile(script)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "words = input(\"Insert claim 1 or description text here:  \")\n",
        "\n",
        "\n",
        "\n",
        "doc = nlp(words)\n",
        "\n",
        "#for ent in doc.ents:\n",
        "#  print(\"ents: \", ent.text, \" \", ent.label_)\n",
        "\n",
        "#nlp.tokenizer.add_special_case('-', [{ORTH: '-', POS:ADP}])\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "#pattern 00,01,10,11 : referanslar için (ör: a chassis, the chassis, vehicle chassis)\n",
        "pattern00 = [{'POS': 'DET', 'OP': '+'},{'POS':'NOUN'},{'LIKE_NUM': True}]\n",
        "pattern01 = [{'POS':'NOUN'},{'POS':'NOUN'},{'LIKE_NUM': True}]\n",
        "pattern02 = [{'POS':'ADJ'},{'POS':'NOUN'},{'LIKE_NUM': True}]\n",
        "pattern10 = [{'POS': 'DET', 'OP': '+'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True},{'TEXT':')'}]\n",
        "pattern11 = [{'POS':'NOUN'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True},{'TEXT':')'}]\n",
        "pattern12 = [{'POS':'ADJ'},{'POS':'NOUN'},{'TEXT':'('},{'LIKE_NUM': True},{'TEXT':')'}]\n",
        "\n",
        "#pattern 2,3,4 : noun ve noun_phrase için sonrasında frekans belirlenerek SEARCH query keyword belirlemede kullanılıyor\n",
        "\n",
        "pattern2 = [{'LIKE_NUM': True,'IS_DIGIT': False}, {'POS':'NOUN'}]\n",
        "pattern3 = [{'POS':'NOUN'},{'POS':'NOUN'}]\n",
        "#pattern4 = [{'POS':'ADJ'},{'POS':'NOUN'}]\n",
        "\n",
        "matcher.add('REFERENCE',None,pattern00,pattern01,pattern02,pattern10,pattern11,pattern12)\n",
        "found_matches = matcher(doc)\n",
        "\n",
        "print(\"\\nPOSSIBLE REFERENCES : \\n\")\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "if len(found_matches)==0:\n",
        "  print(\"NO POSSIBLE REFERENCE found with given format\\n\")\n",
        "\n",
        "referenceList = [doc[start:end].text for match_id, start, end in found_matches]\n",
        "referenceList_freq = Counter(referenceList)\n",
        "common_words = referenceList_freq.most_common(40)\n",
        "print(\"\\nPOSSIBLE REFERENCES (reference_phrase,freq): \", common_words,\"\\n\")\n",
        "\n",
        "print(\"SORTED POSSIBLE REFERENCES REGARDING FREQUENCY IN DESCENDING ORDER: \\n\")\n",
        "\n",
        "sorted_reference_list = [word for (word, freq) in common_words]\n",
        "for i in range(0,len(sorted_reference_list)):\n",
        "  print(str(i+1)+\")-  \"+sorted_reference_list[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "matcher.remove('REFERENCE')\n",
        "matcher.add('NOUN_PHRASE',None,pattern2,pattern3)\n",
        "found_matches = matcher(doc)\n",
        "print(\"POSSIBLE NOUN PHRASES:\\n\")\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text)\n",
        "    \n",
        "    \n",
        "startDocText = [doc[start:end].text for match_id, start, end in found_matches]\n",
        "\n",
        "\n",
        "unexpected_element=[]\n",
        "try:\n",
        "  for i in range(0,len(startDocText)):\n",
        "    if \"-\" in startDocText[i]:\n",
        "      #print(\"unexpected element = \")\n",
        "      unexpected_element.append(startDocText[i])\n",
        "except IndexError:\n",
        "  print(\"ERROR:index out of range\")\n",
        "\n",
        "print(\"List for unexpected element:\",unexpected_element)\n",
        "\n",
        "startDocText = [x for x in startDocText if x not in unexpected_element]\n",
        "\n",
        "\n",
        "print(\"\\nCRITICAL IPC KEYWORDS (comprising system,arrangement,product,device,method etc.):\\n\")\n",
        "\n",
        "try:\n",
        "    #if (startDocText[i][0] != '-' or startDocText[i][-1] != '-'):\n",
        "  for i in range(len(startDocText)):\n",
        "    if (startDocText[i].split()[1].lower()== \"system\" or\n",
        "        startDocText[i].split()[1].lower()==\"arrangement\" or\n",
        "        startDocText[i].split()[1].lower()==\"arangement\" or startDocText[i].split()[1].lower()==\"apparatus\" or\n",
        "        startDocText[i].split()[1].lower()==\"aparatus\" or startDocText[i].split()[1].lower()==\"device\" or\n",
        "        startDocText[i].split()[1].lower()==\"devices\" or\n",
        "        startDocText[i].split()[1].lower()==\"method\" or startDocText[i].split()[1].lower()==\"product\" or \n",
        "        startDocText[i].split()[1].lower()==\"production\" or startDocText[i].split()[1].lower()==\"assembly\" or\n",
        "        startDocText[i].split()[1].lower()==\"asembly\") :\n",
        "      print(startDocText[i])\n",
        "      #critical_firstword=startDocText[i]\n",
        "\n",
        "except: \n",
        "  print(\"AN EXCEPTION OCCURED. UNEXPECTED CHARACTER-INDEX ERROR\\n\")\n",
        "\n",
        "  \n",
        "startDocText_freq = Counter(startDocText)\n",
        "common_words = startDocText_freq.most_common(40)\n",
        "print(\"\\nIPC KEYWORDS (noun_phrase,freq): \", common_words,\"\\n\")\n",
        "\n",
        "#*************** common_words liste lengthi 2'den küçükse \"list index out of range\" hatası veriyor. Dolayısıyla for ile len(list) şeklinde sınırlandırmak doğru olabilir\n",
        "print(\"SORTED IPC KEYWORDS QUERY FORMAT REGARDING FREQUENCY IN DESCENDING ORDER:\\n\")\n",
        "\n",
        "query_words=[]\n",
        "try:\n",
        "  for i in range(0,len(common_words)):\n",
        "    #print(startDocText[i][0],startDocText[i][-1])\n",
        "    #if (startDocText[i][0] != '-' or startDocText[i][-1] != '-'):\n",
        "    phrase_firstword = common_words[i][0].split()[0].lower()\n",
        "    phrase_secondword = common_words[i][0].split()[1].lower()\n",
        "    if (phrase_secondword[-1]==\"s\"):\n",
        "      phrase_secondword = common_words[i][0].split()[1][:-1].lower()\n",
        "    query_word = phrase_firstword+\"_\"+phrase_secondword+\"+\"\n",
        "    print(str(i+1)+\")-  \"+query_word)\n",
        "    query_words.append(query_word)\n",
        "except:\n",
        "  print('ERROR - UNEXPECTED CHARACTER-INDEX ERROR\\n')\n",
        "\n",
        "\n",
        "unique_keyword_list = [word for (word, freq) in common_words if freq is 1]\n",
        "print(\"\\nUNIQUE KEYWORDS (freq=1): \\n\")\n",
        "if (len(unique_keyword_list) ==0):\n",
        "  print(\"NO UNIQUE KEYWORDS FOUND\")\n",
        "else:\n",
        "  for i in range(len(unique_keyword_list)):\n",
        "    print(unique_keyword_list[i])\n",
        "\n",
        "\n",
        "#print(\"\\nCRITICAL IPC KEYWORDS (comprising system,arrangement,product,device,method etc.):\\n\")\n",
        "#for i in range(0,len(critical_keyword_list)):\n",
        " # if (critical_keyword_list[i].split()[1].lower()== \"system\" or\n",
        "  #    critical_keyword_list[i].split()[1].lower()==\"arrangement\" or\n",
        "   #   critical_keyword_list[i].split()[1].lower()==\"arangement\" or critical_keyword_list[i].split()[1].lower()==\"apparatus\" or\n",
        "    #  critical_keyword_list[i].split()[1].lower()==\"aparatus\" or critical_keyword_list[i].split()[1].lower()==\"device\" or\n",
        "     # critical_keyword_list[i].split()[1].lower()==\"method\" or critical_keyword_list[i].split()[1].lower()==\"product\" or \n",
        "      #critical_keyword_list[i].split()[1].lower()==\"production\" or critical_keyword_list[i].split()[1].lower()==\"assembly\" or\n",
        "      #critical_keyword_list[i].split()[1].lower()==\"asembly\"):\n",
        "    #print(critical_keyword_list[i])\n",
        " \n",
        "  \n",
        "print(\"\\nPROBABLE PATENW QUERY FOR IPC OR SIMILAR DOCUMENTS : \\n\")\n",
        "patenw_query = \" \"\n",
        "if (len(query_words) > 3):\n",
        "  for i in range(0,3):\n",
        "    patenw_query += query_words[i]+\",\"\n",
        "  print(\"and\"+patenw_query[:-1])\n",
        "\n",
        "elif (len(query_words) > 0):\n",
        "  for i in range(0,len(query_words)):\n",
        "    patenw_query += query_words[i]+\",\"\n",
        "  print(\"and\"+patenw_query[:-1])\n",
        "\n",
        "else: \n",
        "  print(\"\"\"NO NOUN PHRASE FOUND... NO RESULTS FOR QUERY !\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0K7rV-cdgIa",
        "colab_type": "code",
        "outputId": "4d88e4c1-1693-4623-9bd1-d8190a8aeb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "wordforSimilarity= input(\"Insert a word for similar matches: \")\n",
        "wordforSimilarity = wordforSimilarity.lower()\n",
        "new_vector = nlp.vocab[wordforSimilarity].vector\n",
        "\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors and mixed-case words:\n",
        "    if word.vector_norm > 0:\n",
        "        if word.is_lower:\n",
        "          if word.is_alpha:\n",
        "            similarity = cosine_similarity(new_vector, word.vector)\n",
        "            computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "\n",
        "print([w[0].text for w in computed_similarities[:20]])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Insert a word for similar matches: WORD\n",
            "['word', 'phrase', 'words', 'meaning', 'phrases', 'language', 'spoken', 'dictionary', 'spelling', 'referring', 'means', 'saying', 'verb', 'term', 'speak', 'sentence', 'spelled', 'meant', 'name', 'speaking']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeWfHSrK-8FK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC-PREDICT-NOUN PHRASE\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "\n",
        "input_doc = nlp(my_input)\n",
        "\n",
        "#for ent in doc.ents:\n",
        "#  print(\"ents: \", ent.text, \" \", ent.label_)\n",
        "\n",
        "#nlp.tokenizer.add_special_case('-', [{ORTH: '-', POS:ADP}])\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "#pattern 00,01,10,11 : referanslar için (ör: a chassis, the chassis, vehicle chassis)\n",
        "patternipc1 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc2 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc3 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'ADJ'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc4 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'ADJ'},{'POS':'NOUN'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "patternipc5 = [{'POS': 'NOUN', 'OP': '!'},{'POS':'NOUN'},{'POS': 'NOUN', 'OP': '!'}]\n",
        "matcher.add('IPC',None,patternipc1,patternipc2,patternipc3,patternipc4,patternipc5)\n",
        "found_matches = matcher(input_doc)\n",
        "\n",
        "print('\\nPOSSIBLEIPC WORDS:\\n')\n",
        "if len(found_matches)==0:\n",
        "  print(\"NO POSSIBLE IPC WORD found with given format\\n\")\n",
        "\n",
        "list_ipcwords = []\n",
        "for match_id, start, end in found_matches:\n",
        "  string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "  span = input_doc[start:end]                    # get the matched span\n",
        "  print(match_id, string_id, start, end, span.text)\n",
        "  list_ipcwords.append(span.text)\n",
        "print('\\n')\n",
        "\n",
        "ipctext = ' '.join(list_ipcwords)\n",
        "rawdoc = nlp(ipctext)\n",
        "apptext=[]\n",
        "for token in rawdoc:\n",
        "  if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
        "    apptext.append(token.text)\n",
        "# unique ifadeler bulunup listeye dahil edilir, listeden tek bir string(unique_text) üretilir,bu string de nlp ile doc objesine dönüştürülür \n",
        "unique_list = [] \n",
        "for x in apptext:\n",
        "  if x not in unique_list: \n",
        "    unique_list.append(x) \n",
        "\n",
        "unique_text = ' '.join(unique_list)\n",
        "appdoc = nlp(unique_text)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ipc.tsv', sep='\\t')\n",
        "\n",
        "# ilk cümle başarısız olursa alttaki 2 yorum koduna devam edilebilir\n",
        "#app = input(\"insert critical words for ipc: \")\n",
        "#appdoc= nlp(app)\n",
        "probIpc=[]\n",
        "for i in(range(len(df['Definition']))):\n",
        "  print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "\n",
        "print(\"FIRST FIVE MAXIMUM VALUES:\\n\")\n",
        "\n",
        "df['predictionValue']=pd.Series(probIpc)\n",
        "print(df.sort_values(by='predictionValue', ascending = False))\n",
        "print(\"\\n\")\n",
        "\n",
        "try:\n",
        "  print(\"VALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "except:\n",
        "  print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "\n",
        "answer=input(\"\\nIf you are not satisfied with the results do you want to insert your own noun phrases for ipc ?\\n\")\n",
        "\n",
        "if answer.lower() == \"y\" or answer.lower() == \"yes\":\n",
        "  app = input(\"Insert critical noun phrases for ipc wihout punctuation (e.g image processor, marine, communication network ): \")\n",
        "  appdoc= nlp(app)\n",
        "  probIpc=[]\n",
        "  for i in(range(len(df['Definition']))):\n",
        "    print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "    probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  print(\"\\nMAXIMUM VALUES:\\n\")\n",
        "  df['predictionValue']=pd.Series(probIpc)\n",
        "  print(df.sort_values(by='predictionValue', ascending = False).head(20))\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  try:\n",
        "    print(\"VALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "    print(\"\\nYou can run again for new search...\")\n",
        "  except:\n",
        "    print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "else:\n",
        "  print(\"\\nYou can run again for new search...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KoLmTHHqon7u",
        "colab": {}
      },
      "source": [
        "df.sort_values(by='predictionValue', ascending = False).head(40)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iT2NSukytjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail(60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHr7fl-fsmf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app=input(\"app: \")\n",
        "ipc=input(\"ipc: \")\n",
        "docipc = nlp(ipc)\n",
        "docapp = nlp(app)\n",
        "print(docapp.vector_norm,docipc.vector_norm)\n",
        "print(\"app:\",docapp.has_vector)\n",
        "print(\"ipc:\",docipc.has_vector)\n",
        "print(docapp.similarity(docipc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyWwAMHtPOPr",
        "colab_type": "code",
        "outputId": "98e3981c-096c-465e-dd1b-f01fad154d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "doc=nlp(\"an airbag deopleyment system is provided\")\n",
        "for chunk in doc.noun_chunks:\n",
        "  liste=[chunk.text for chunk in doc.noun_chunks]\n",
        "print(\"liste: \",liste)\n",
        "for token in doc:\n",
        "  print(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "liste:  ['an airbag deopleyment system']\n",
            "an an DET DT det xx True True\n",
            "airbag airbag NOUN NN compound xxxx True False\n",
            "deopleyment deopleyment NOUN NN compound xxxx True False\n",
            "system system NOUN NN nsubjpass xxxx True False\n",
            "is be VERB VBZ auxpass xx True True\n",
            "provided provide VERB VBN ROOT xxxx True False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhtaEqr6Qw0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IPC PREDICT-NOUN-CHUNKS (+ (TOKEN.TAG_=VBG) ?)\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "#--------\n",
        "input_doc = nlp(my_input)\n",
        "liste_chunk=[]\n",
        "for chunk in input_doc.noun_chunks:\n",
        "  print(\"CHUNK: \",chunk)\n",
        "  for i in(range(len(chunk))):\n",
        "    if (chunk[i].is_stop == False and chunk[i].text != \"-\" and chunk[i].is_punct == False and chunk[i].pos_ != 'NUM'):\n",
        "      liste_chunk.append(chunk[i].text)\n",
        "#---------\n",
        "\n",
        "print(\"CHUNK LİSTESİ: \",liste_chunk)\n",
        "\n",
        "ipctext = ' '.join(liste_chunk)\n",
        "\n",
        "\n",
        "appdoc = nlp(ipctext)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ipc.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "probIpc=[]\n",
        "for i in(range(len(df['Definition']))):\n",
        "  print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES: \\n\")\n",
        "\n",
        "df['predictionValue']=pd.Series(probIpc)\n",
        "print(df.sort_values([\"predictionValue\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#burası çok önemli yeni df'nin indexi sıfırlanmalı\n",
        "df2 = df.sort_values([\"predictionValue\"], ascending = False)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "\n",
        "#most prop ipc list, df2'deki IPC verileri ile oluşturulur\n",
        "most_prob_ipc_list=[]\n",
        "for i in range(0,20):\n",
        "  most_prob_ipc_list.append((df2['IPC'][i],i))\n",
        "\n",
        "final_ipc_df1 = pd.DataFrame(most_prob_ipc_list, columns =['IPC','FIRST-INDEX'])\n",
        "#print(\"MOST PROBABLE IPC LIST: \\n\",most_prob_ipc_list)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"\\nVALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "except:\n",
        "  print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "\n",
        "answer=input(\"\\nIf you are not happy with the results do you want to insert your own noun phrases for ipc ?\\n\")\n",
        "\n",
        "if answer.lower() == \"y\" or answer.lower() == \"yes\":\n",
        "  app = input(\"Insert critical noun phrases for ipc wihout punctuation (e.g image processor, marine, communication network ): \")\n",
        "  appdoc= nlp(app)\n",
        "  probIpc=[]\n",
        "  for i in(range(len(df['Definition']))):\n",
        "    print(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "    probIpc.append(appdoc.similarity(nlp(str(df['Definition'][i]))))\n",
        "  print(\"\\nMAXIMUM VALUES:\\n\")\n",
        "  df['predictionValue']=pd.Series(probIpc)\n",
        "  print(df.sort_values([\"predictionValue\"], ascending = False).head(20))\n",
        "\n",
        "  print(\"\\nFIRST PREDICTION VALUES:\\n\", df2.head(20))\n",
        "  \n",
        "  # ilk df2'dn farklı olarak ikinci df2 oluşturulur sonuçta her iki df2'deki IPC verileri ile most prob ipc list oluşur\n",
        "  df2=df.sort_values([\"predictionValue\"], ascending = False)\n",
        "  df2 = df2.reset_index(drop=True)\n",
        "\n",
        "  most_prob_ipc_list2=[]\n",
        "  for m in range(0,20):\n",
        "    most_prob_ipc_list2.append((df2['IPC'][m],m))\n",
        "  #print(\"MOST PROBABLE IPC LIST: \\n\",most_prob_ipc_list)\n",
        "  final_ipc_df2 = pd.DataFrame(most_prob_ipc_list2, columns =['IPC','SECOND-INDEX'])\n",
        "  result_df = pd.merge(final_ipc_df1,final_ipc_df2, on='IPC', how='outer', indicator=True)\n",
        "\n",
        "  #result_df = pd.merge(final_ipc_df1,final_ipc_df2, on='IPC')\n",
        "  #result_df = final_ipc_df1.append(final_ipc_df2, sort=False)\n",
        "  print(\"\\nRESULT:\\n\",result_df)\n",
        "  #final_ipc_list = [(x, most_prob_ipc_list.count(x),most_prob_ipc_list.) for x in set(most_prob_ipc_list)]\n",
        "  #print (\"IPC FREQUENCY IN BOTH METHODS: \", final_ipc_list) \n",
        "  \n",
        "  #final_ipc_df = pd.DataFrame(final_ipc_list, columns =['IPC', 'FREQUENCY', 'FIRST-INDEX']) \n",
        "  #-----alttaki komut önemli, son sorting\n",
        "  result_df['TOTAL-INDEX'] = result_df['FIRST-INDEX']+result_df['SECOND-INDEX']\n",
        "\n",
        "  final_sorted_ipc_df = result_df.sort_values([\"TOTAL-INDEX\"], ascending = True)\n",
        "  print(\"\\nIPC FREQUENCY IN BOTH METHODS:\\n \",final_sorted_ipc_df)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nVALUES GREATER THAN 0.5:\\n\",df[df['predictionValue'] > 0.5])\n",
        "    print(\"\\nYou can run again for new search...\")\n",
        "  except:\n",
        "    print('NO PREDICTION VALUE GREATER THAN 0.5\\n')\n",
        "\n",
        "else:\n",
        "  print(\"\\nYou can run again for new search...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3n2ViiFqrfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2243db9e-8f79-4119-e936-53a10e53b127"
      },
      "source": [
        "#IPC PREDICT-NOUN-CHUNKS (+ (TOKEN.TAG_=VBG) ?)\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from scipy import spatial\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "my_input = input(\"Insert the first meaningful sentence in description regarding ipc: \")\n",
        "my_input=my_input.lower()\n",
        "\n",
        "#--------\n",
        "input_doc = nlp(my_input)\n",
        "liste_chunk=[]\n",
        "for chunk in input_doc.noun_chunks:\n",
        "  print(\"CHUNK: \",chunk)\n",
        "  for i in(range(len(chunk))):\n",
        "    if (chunk[i].is_stop == False and chunk[i].text != \"-\" and chunk[i].is_punct == False and chunk[i].pos_ != 'NUM'):\n",
        "      liste_chunk.append(chunk[i].text)\n",
        "#---------\n",
        "\n",
        "print(\"CHUNK LİSTESİ: \",liste_chunk)\n",
        "\n",
        "ipctext = ' '.join(liste_chunk)\n",
        "\n",
        "\n",
        "appdoc = nlp(ipctext)\n",
        "print(\"\\nAPPDOC: \",appdoc)\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/FULLIPC-22-11.tsv', sep='\\t')\n",
        "\n",
        "#BURADA DEF1 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC1 LİSTESİNE EKLENİR. DEF1 4 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc1=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "  probIpc1.append(appdoc.similarity(nlp(str(df['DEF1'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF1: \\n\")\n",
        "\n",
        "df['predictionValue1']=pd.Series(probIpc1)\n",
        "print(df.sort_values([\"predictionValue1\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#BURADA DEF2 SÜTUNU İLE BENZERLİK DEĞERLERİ PROBIPC2 LİSTESİNE EKLENİR. DEF2 3 HANELİ IPC SINIFI İÇİN TANIMDIR\n",
        "probIpc2=[]\n",
        "for i in(range(len(df['IPC']))):\n",
        "  #print(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "  probIpc2.append(appdoc.similarity(nlp(str(df['DEF2'][i]))))\n",
        "\n",
        "print(\"\\nMAXIMUM VALUES FOR DEF2: \\n\")\n",
        "\n",
        "df['predictionValue2']=pd.Series(probIpc2)\n",
        "print(df.sort_values([\"predictionValue2\"], ascending = False).head(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "#burası çok önemli yeni df'nin indexi sıfırlanmalı\n",
        "df['FULLIPC-Prediction']=df['predictionValue1']+df['predictionValue2']\n",
        "df2 = df.sort_values([\"FULLIPC-Prediction\"], ascending = False)\n",
        "df2 = df2.reset_index(drop=True)\n",
        "#---------------------\n",
        "print(\"FULL IPC PREDICTION MAX. VALUES:\\n\",df2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Insert the first meaningful sentence in description regarding ipc: The present invention is directed to an improved high beam headlight indicator and, more specifically, to a high beam headlight indicator mounted dead center on the top of the steering wheel facing the driver of a vehicle\n",
            "CHUNK:  the present invention\n",
            "CHUNK:  an improved high beam headlight indicator\n",
            "CHUNK:  a high beam headlight indicator\n",
            "CHUNK:  dead center\n",
            "CHUNK:  the top\n",
            "CHUNK:  the steering wheel\n",
            "CHUNK:  the driver\n",
            "CHUNK:  a vehicle\n",
            "CHUNK LİSTESİ:  ['present', 'invention', 'improved', 'high', 'beam', 'headlight', 'indicator', 'high', 'beam', 'headlight', 'indicator', 'dead', 'center', 'steering', 'wheel', 'driver', 'vehicle']\n",
            "\n",
            "APPDOC:  present invention improved high beam headlight indicator high beam headlight indicator dead center steering wheel driver vehicle\n",
            "\n",
            "MAXIMUM VALUES FOR DEF1: \n",
            "\n",
            "     IPC  ... predictionValue1\n",
            "12  B60Q  ...         0.709329\n",
            "15  B60T  ...         0.707133\n",
            "13  B60R  ...         0.705890\n",
            "2   B60D  ...         0.647950\n",
            "17  B60W  ...         0.629400\n",
            "9   B60M  ...         0.626871\n",
            "7   B60K  ...         0.618647\n",
            "14  B60S  ...         0.617500\n",
            "0   B60B  ...         0.601850\n",
            "16  B60V  ...         0.597591\n",
            "11  B60P  ...         0.572508\n",
            "8   B60L  ...         0.538676\n",
            "5   B60H  ...         0.537597\n",
            "1   B60C  ...         0.532171\n",
            "4   B60G  ...         0.495495\n",
            "10  B60N  ...         0.448045\n",
            "6   B60J  ...         0.437334\n",
            "3   B60F  ...         0.135592\n",
            "\n",
            "[18 rows x 12 columns]\n",
            "\n",
            "\n",
            "\n",
            "MAXIMUM VALUES FOR DEF2: \n",
            "\n",
            "     IPC  ... predictionValue2\n",
            "0   B60B  ...         0.763005\n",
            "1   B60C  ...         0.763005\n",
            "16  B60V  ...         0.763005\n",
            "15  B60T  ...         0.763005\n",
            "14  B60S  ...         0.763005\n",
            "13  B60R  ...         0.763005\n",
            "12  B60Q  ...         0.763005\n",
            "11  B60P  ...         0.763005\n",
            "10  B60N  ...         0.763005\n",
            "9   B60M  ...         0.763005\n",
            "8   B60L  ...         0.763005\n",
            "7   B60K  ...         0.763005\n",
            "6   B60J  ...         0.763005\n",
            "5   B60H  ...         0.763005\n",
            "4   B60G  ...         0.763005\n",
            "3   B60F  ...         0.763005\n",
            "2   B60D  ...         0.763005\n",
            "17  B60W  ...         0.763005\n",
            "\n",
            "[18 rows x 13 columns]\n",
            "\n",
            "\n",
            "FULL IPC PREDICTION MAX. VALUES:\n",
            "      IPC  ... FULLIPC-Prediction\n",
            "0   B60Q  ...           1.472335\n",
            "1   B60T  ...           1.470138\n",
            "2   B60R  ...           1.468895\n",
            "3   B60D  ...           1.410955\n",
            "4   B60W  ...           1.392405\n",
            "5   B60M  ...           1.389877\n",
            "6   B60K  ...           1.381653\n",
            "7   B60S  ...           1.380505\n",
            "8   B60B  ...           1.364855\n",
            "9   B60V  ...           1.360596\n",
            "10  B60P  ...           1.335513\n",
            "11  B60L  ...           1.301681\n",
            "12  B60H  ...           1.300603\n",
            "13  B60C  ...           1.295176\n",
            "14  B60G  ...           1.258500\n",
            "15  B60N  ...           1.211050\n",
            "16  B60J  ...           1.200339\n",
            "17  B60F  ...           0.898597\n",
            "\n",
            "[18 rows x 14 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}